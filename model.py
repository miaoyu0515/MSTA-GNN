#!/usr/bin/env python36
# -*- coding: utf-8 -*-
"""
Created on July, 2018

@author: Tangrizzly
"""

import datetime
import math
from math import sqrt
import numpy as np
import torch
from torch import nn
from torch.nn import Module, Parameter
import torch.nn.functional as F
import torch.nn.init as init
from numba import jit
from entmax import entmax_bisect
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from recbole.config import Config
from mamba_ssm import Mamba  # ä»mamba_ssmå—ä¸­å¯¼å…¥ä¸€ä¸ªMambaå‡½æ•°
from temporal_attention_layer import TA_layer

# ç”¨äºå®ç°å±‚å½’ä¸€åŒ–
class LayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-12):
        """Construct a layernorm module in the TF style (epsilon inside the square root).
        """
        super(LayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.variance_epsilon = eps

    def forward(self, x):
        u = x.mean(-1, keepdim=True)  # è®¡ç®—è¾“å…¥xçš„å‡å€¼
        # è®¡ç®—xçš„æ–¹å·®
        s = (x - u).pow(2).mean(-1, keepdim=True)
        # å½’ä¸€åŒ–ï¼šä½¿xå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
        x = (x - u) / torch.sqrt(s + self.variance_epsilon)
        return self.weight * x + self.bias


# GCNå®šä¹‰
# ä¸€èˆ¬è¾“å…¥ä¸ºï¼ˆbatch_sizaï¼Œinput_size)
class GraphConvolution(Module):
    # (è¾“å…¥ç‰¹å¾æ•°ï¼Œè¾“å‡ºç‰¹å¾æ•°ï¼Œæ˜¯å¦æœ‰åç½®ï¼‰
    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features * 2, out_features * 2))  # ç»´åº¦ä»inå˜ä¸ºout
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features * 2))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    # ï¼ˆç»™æƒé‡å’Œåç½®èµ‹äºˆåˆå§‹å€¼ï¼‰
    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    # å‰å‘ä¼ æ’­ï¼ˆè¾“å…¥æ•°æ®ï¼Œé‚»æ¥çŸ©é˜µï¼‰# [b,s,d]#[batch_size, batch_size]
    def forward(self, input, adj):
        # print(input.size())
        # print(self.weight.size())
        # print(input.size())
        batch_size, seq_len, de_size = input.size()
        # å°†è¾“å…¥å¼ é‡å½¢çŠ¶é‡å¡‘
        support = input.view(batch_size * seq_len, -1).matmul(self.weight)
        support = support.view(batch_size, seq_len, -1)  # æ¢å¤batchç»´åº¦
        # print(support.size())
        # print(adj.size())
        # åº”ç”¨é‚»æ¥çŸ©é˜µè¿›è¡Œç¨€ç–çŸ©é˜µä¹˜æ³•
        # output = torch.matmul(support, adj)
        output = torch.matmul(adj, support)

        # support = torch.mm(input, self.weight)  # è¾“å‡ºç»´åº¦ç”±self.weightçš„åˆ—æ•°å†³å®š
        # output = torch.spmm(adj, support)
        if self.bias is not None:  # biasä¸å½±å“ç»´åº¦å˜åŒ–
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
            + str(self.in_features) + ' -> ' \
            + str(self.out_features) + ')'


class GNN(nn.Module):
    # self.gnn = GNN(self.hidden_size, 0.5)
    # def __init__(self, batch, hidden, dropout):
    def __init__(self, hidden, dropout):
        super(GNN, self).__init__()
        # å®šä¹‰ç¬¬ä¸€å±‚å›¾å·ç§¯
        self.gc1 = GraphConvolution(hidden, hidden)
        # å®šä¹‰ç¬¬äºŒå±‚å›¾å·ç§¯
        self.gc2 = GraphConvolution(hidden, hidden)
        # æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        self.TA = TA_layer(hidden*2, hidden*2, 2, 2)
        self.dropout = dropout
        # self.batch = batch

    # hidden = self.gnn(seq_emb, A)  # (b,s,2d)
    def forward(self, x, adj):
        # for batch_index in range(x.size(0)):  # éå† batch ç»´åº¦
        # è·å–å½“å‰ batch çš„ hidden x hidden çŸ©é˜µ
        # batch_matrix = x[batch_index]
        # ç¬¬ä¸€å±‚å·ç§¯åï¼‹ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.gc1(x, adj))
        # åˆ©ç”¨dropout
        x = F.dropout(x, self.dropout, training=self.training)
        # ç¬¬äºŒå±‚å·ç§¯
        x = self.gc2(x, adj)
         # æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        x = self.TA(x)
        return x


# class Attention(nn.Module):
#     # ï¼ˆè¾“å…¥ç»´åº¦ï¼Œéšå±‚ç»´åº¦ï¼‰
#     def __init__(self, in_size, hidden_size=200):
#         super(Attention, self).__init__()
#         # åºåˆ—æ¨¡å‹ï¼ˆä¸¤ä¸ªçº¿æ€§å±‚ï¼‹Tanhæ¿€æ´»å‡½æ•°ï¼‰
#         self.project = nn.Sequential(
#             nn.Linear(in_size * 2, hidden_size),
#             nn.Tanh(),
#             nn.Linear(hidden_size, 1, bias=False)
#         )
#
#     def forward(self, z):
#         # print(z.size())
#         w = self.project(z)
#         # print(w.size())
#         # è·å¾—æ³¨æ„åŠ›æƒé‡
#         beta = torch.softmax(w, dim=1)
#         return (beta * z).sum(1), beta
#
#
# class GNN(nn.Module):
#     def __init__(self, hidden_size, dropout):
#         super(GNN, self).__init__()
#         # ä¸‰å±‚å·ç§¯ç½‘ç»œçš„å®šä¹‰
#         # self.gnn = GNN(self.hidden_size, 0.5)
#         self.dropout = dropout
#         self.SGCN1 = GCN(hidden_size, dropout)
#         self.SGCN2 = GCN(hidden_size, dropout)
#         # ï¼ˆå†…å®¹å·ç§¯ç½‘ç»œï¼‰
#         self.CGCN = GCN(hidden_size, dropout)
#
#         # å®šä¹‰æ³¨æ„åŠ›å‚æ•°
#         self.a = nn.Parameter(torch.zeros(size=(hidden_size, 1)))
#         nn.init.xavier_uniform_(self.a.data, gain=1.414)
#         # æ³¨æ„åŠ›å±‚
#         self.attention = Attention(hidden_size)
#         self.tanh = nn.Tanh()
#         # åºåˆ—æ¨¡å‹çš„å®šä¹‰
#         self.MLP = nn.Sequential(
#             nn.Linear(hidden_size * 2, hidden_size * 2),
#             nn.LogSoftmax(dim=1)
#         )
#
#     def forward(self, x, adj):
#         # print(x.size())
#         emb1 = self.SGCN1(x, adj)
#         com1 = self.CGCN(x, adj)
#         com2 = self.CGCN(x, adj)
#         emb2 = self.SGCN2(x, adj)
#         Xcom = (com1 + com2) / 2
#         # print(emb2.size())
#         # print(com1.size())
#         emb = torch.stack([emb1, emb2, Xcom], dim=1)
#         # print('666')
#         # print(emb.size())
#         # é€šè¿‡æ³¨æ„åŠ›å±‚
#         emb, att = self.attention(emb)
#         # print(emb.size())
#         # print(emb.size())
#         output = self.MLP(emb)
#         # print(output.size())
#         return output
#         # return output, att, emb1, com1, com2, emb2, emb
#

# # æŸ¥æ‰¾é‚»å±…èŠ‚ç‚¹
class FindNeighbors(Module):
    def __init__(self, hidden_size,nei_n):
        super(FindNeighbors, self).__init__()
        self.hidden_size = hidden_size
        self.neighbor_n = nei_n # Diginetica:3; Tmall: 7; Nowplaying: 4
        self.dropout40 = nn.Dropout(0.40)

    # è®¡ç®—ä¼šè¯åµŒå…¥å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦
    def compute_sim(self, sess_emb):
        fenzi = torch.matmul(sess_emb, sess_emb.permute(1, 0))
        fenmu_l = torch.sum(sess_emb * sess_emb + 0.000001, 1)
        fenmu_l = torch.sqrt(fenmu_l).unsqueeze(1)
        fenmu = torch.matmul(fenmu_l, fenmu_l.permute(1, 0))
        cos_sim = fenzi / fenmu
        cos_sim = nn.Softmax(dim=-1)(cos_sim)
        return cos_sim

    def forward(self, sess_emb):
        k_v = self.neighbor_n  # å­˜å‚¨é‚»å±…çš„æ•°é‡
        cos_sim = self.compute_sim(sess_emb)  # è®¡ç®—ç›¸ä¼¼åº¦
        # å¦‚æœä¼šè¯æ•°é‡å°äºé‚»å±…æ•°ï¼Œåˆ™è°ƒæ•´é‚»å±…æ•°
        if cos_sim.size()[0] < k_v:
            k_v = cos_sim.size()[0]
        # ä½¿ç”¨topkæ–¹æ³•æ‰¾åˆ°æ¯ä¸ªä¼šè¯çš„kä¸ªæœ€ç›¸ä¼¼é‚»å±…çš„ç›¸ä¼¼åº¦
        cos_topk, topk_indice = torch.topk(cos_sim, k=k_v, dim=1)
        cos_topk = nn.Softmax(dim=-1)(cos_topk)
        # æ ¹æ®ç´¢å¼•è·å–æ¯ä¸ªä¼šè¯kä¸ªæœ€ç›¸ä¼¼é‚»å±…çš„åµŒå…¥å‘é‡
        sess_topk = sess_emb[topk_indice]

        cos_sim = cos_topk.unsqueeze(2).expand(cos_topk.size()[0], cos_topk.size()[1], self.hidden_size)
        #  è®¡ç®—é‚»å±…ä¼šè¯çš„åŠ æƒå’Œ
        neighbor_sess = torch.sum(cos_sim * sess_topk, 1)
        # åº”ç”¨dropoutå±‚
        neighbor_sess = self.dropout40(neighbor_sess)  # [b,d]
        # è¿”å›ç»è¿‡dropoutå¤„ç†çš„ä¼šè¯åµŒå…¥å‘é‡
        return neighbor_sess


# mambaæ¨¡å—
class Mamba4Rec(nn.Module):
    # def __init__(self, opt, config, dataset):
    def __init__(self, opt, config):
        super(Mamba4Rec, self).__init__()
        # ä»é…ç½®ä¸­è·å–æ¨¡å‹çš„è¶…å‚æ•°
        self.hidden_size = opt.hiddenSize
        self.loss_type = config["loss_type"]
        self.num_layers = config["num_layers"]
        self.dropout_prob = config["dropout_prob"]
        # self.hidden_size = config["hidden_size"]
        # self.loss_type = config["loss_type"]
        # self.num_layers = config["num_layers"]
        # self.dropout_prob = config["dropout_prob"]

        # mambaå—çš„è¶…å‚æ•°
        self.d_state = config["d_state"]
        self.d_conv = config["d_conv"]
        self.expand = config["expand"]

        # å®šä¹‰åµŒå…¥å±‚
        # å°†ç‰©å“IDè½¬æ¢ä¸ºåµŒå…¥å‘é‡
        # self.item_embedding = nn.Embedding(
        #     self.n_items, self.hidden_size, padding_idx=0
        # )
        # å±‚å½’ä¸€åŒ–
        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=1e-12)
        # dropout
        self.dropout = nn.Dropout(self.dropout_prob)

        # å®šä¹‰mambaå±‚åˆ—è¡¨ï¼šåŒ…å«å¤šä¸ªmambalayer
        self.mamba_layers = nn.ModuleList([
            MambaLayer(
                # d_model=self.hidden_size,
                d_model=200,
                d_state=self.d_state,
                d_conv=self.d_conv,
                expand=self.expand,
                dropout=self.dropout_prob,
                num_layers=self.num_layers,
            ) for _ in range(self.num_layers)
        ])

        # æ ¹æ®æŸå¤±ç±»å‹å®šä¹‰æŸå¤±å‡½æ•°
        if self.loss_type == "BPR":
            self.loss_fct = BPRLoss()
        elif self.loss_type == "CE":
            self.loss_fct = nn.CrossEntropyLoss()
        else:
            raise NotImplementedError("Make sure 'loss_type' in ['BPR', 'CE']!")

        self.apply(self._init_weights)

    # æƒé‡åˆå§‹åŒ–
    def _init_weights(self, module):
        # å¦‚æœæ¨¡å—æ˜¯çº¿æ€§å±‚æˆ–åµŒå…¥å±‚ï¼Œè¿›è¡Œæƒé‡åˆå§‹åŒ–
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
        # å¦‚æœæ˜¯å±‚å½’ä¸€åŒ–ï¼Œåç½®ä¸º0ï¼Œæƒé‡ä¸º1
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        # å¦‚æœæ˜¯å¸¦åç½®çš„çº¿æ€§å±‚ï¼Œåˆå§‹åŒ–åç½®ä¸º0
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()

    def gather_indexes(self, output, gather_index):
        """Gathers the vectors at the specific positions over a minibatch"""
        gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1]).to(output.device)
        output_tensor = output.gather(dim=1, index=gather_index)
        return output_tensor.squeeze(1)

    def forward(self, item_emb, item_seq_len):
        # def forward(self, item_seq, item_seq_len):
        # èŠ‚ç‚¹åµŒå…¥
        # item_emb = self.item_embedding(item_seq)
        # item_emb = self.dropout(item_emb)
        # item_emb = self.LayerNorm(item_emb)

        # é€šè¿‡mambaå±‚å‰å‘ä¼ æ’­
        for i in range(self.num_layers):
            item_emb = self.mamba_layers[i](item_emb)
        c = item_emb[:, -1, :].unsqueeze(1)  # [b,d]->[b,1,d]
        x_n = item_emb[:, :-1, :]  # [b,s,d]
        # # æ ¹æ®åºåˆ—é•¿åº¦è·å–è¾“å‡º
        # seq_output = self.gather_indexes(item_emb, item_seq_len - 1)

        return c, x_n


class MambaLayer(nn.Module):
    # åˆå§‹åŒ–å—çš„å‚æ•°åŠå±‚
    def __init__(self, d_model, d_state, d_conv, expand, dropout, num_layers):
        super().__init__()
        self.num_layers = num_layers
        # åˆ›å»ºmambaæ¨¡å—
        self.mamba = Mamba(
            # This module uses roughly 3 * expand * d_model^2 parameters
            d_model=d_model,
            d_state=d_state,
            d_conv=d_conv,
            expand=expand,
        )
        # åˆ›å»ºdropoutå±‚
        self.dropout = nn.Dropout(dropout)
        # åˆ›å»ºå±‚å½’ä¸€åŒ–
        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)
        # åˆ›å»ºå‰é¦ˆç¥ç»ç½‘ç»œ
        self.ffn = FeedForward(d_model=d_model, inner_size=d_model * 4, dropout=dropout)

    # å‰å‘ä¼ æ’­æ–¹æ³•
    def forward(self, input_tensor):
        # è°ƒç”¨mambaå—å¤„ç†å¼ é‡å¾—åˆ°éšè—çŠ¶æ€
        hidden_states = self.mamba(input_tensor)

        # åˆ¤æ–­mambaå±‚æ•°
        if self.num_layers == 1:  # one Mamba layer without residual connection
            hidden_states = self.LayerNorm(self.dropout(hidden_states))
        else:  # stacked Mamba layers with residual connections
            hidden_states = self.LayerNorm(self.dropout(hidden_states) + input_tensor)
        # é€šè¿‡å‰é¦ˆç¥ç»ç½‘ç»œå¤„ç†éšè—çŠ¶æ€
        hidden_states = self.ffn(hidden_states)
        return hidden_states


class FeedForward(nn.Module):
    def __init__(self, d_model, inner_size, dropout=0.2):
        super().__init__()
        # åˆ›å»ºä¸¤ä¸ªçº¿æ€§å±‚
        self.w_1 = nn.Linear(d_model, inner_size)
        self.w_2 = nn.Linear(inner_size, d_model)
        # åˆ›å»ºæ¿€æ´»å‡½æ•°
        self.activation = nn.GELU()
        self.dropout = nn.Dropout(dropout)
        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)

    def forward(self, input_tensor):
        # ä¾æ¬¡é€šè¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼Œæ¿€æ´»å‡½æ•°ï¼Œdropout
        hidden_states = self.w_1(input_tensor)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.dropout(hidden_states)
        # ä¾æ¬¡é€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼Œdropoutï¼Œå±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
        hidden_states = self.w_2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)

        return hidden_states


class RelationGAT(Module):
    def __init__(self, batch_size, hidden_size=100):
        super(RelationGAT, self).__init__()
        self.batch_size = batch_size
        self.dim = hidden_size
        # å®šä¹‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¾“å…¥ç»´åº¦ä¸º2å€çš„éšè—å±‚ç»´åº¦ï¼Œè¾“å‡ºç»´åº¦ä¸ºéšè—å±‚ç»´åº¦
        self.w_f = nn.Linear(2 * hidden_size, hidden_size)
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡å‚æ•°
        self.alpha_w = nn.Linear(self.dim, 1)
        # æ³¨æ„åŠ›æƒé‡å‚æ•°çŸ©é˜µ
        self.atten_w0 = nn.Parameter(torch.Tensor(1, self.dim))
        self.atten_w1 = nn.Parameter(torch.Tensor(self.dim, self.dim))
        self.atten_w2 = nn.Parameter(torch.Tensor(self.dim, self.dim))
        self.atten_bias = nn.Parameter(torch.Tensor(self.dim))

    # è·å–æ³¨æ„åŠ›æƒé‡
    def get_alpha(self, x=None):
        # x[b,1,d]
        alpha_global = torch.sigmoid(self.alpha_w(x)) + 1  # [b,1,1]
        alpha_global = self.add_value(alpha_global)
        return alpha_global  # [b,1,1]

    # å¯¹è¾“å…¥valueè¿›è¡Œå¤„ç†
    def add_value(self, value):
        mask_value = (value == 1).float()
        value = value.masked_fill(mask_value == 1, 1.00001)
        return value

    # è®¡ç®—ç›®æ ‡å¼ é‡å’Œé”®å€¼å¯¹å¼ é‡ä¹‹é—´çš„æ³¨æ„åŠ›
    def tglobal_attention(self, target, k, v, alpha_ent=1):
        # è®¡ç®—æƒé‡
        alpha = torch.matmul(torch.relu(k.matmul(self.atten_w1) + target.matmul(self.atten_w2) + self.atten_bias),
                             self.atten_w0.t())
        # è°ƒç”¨entmaxå‡½æ•°è°ƒæ•´alphaå€¼
        alpha = entmax_bisect(alpha, alpha_ent, dim=1)
        c = torch.matmul(alpha.transpose(1, 2), v)
        return c

    def forward(self, item_embedding, items, A, D, target_embedding):
        seq_h = []
        for i in torch.arange(items.shape[0]):
            seq_h.append(torch.index_select(item_embedding, 0, items[i]))  # [b,s,d]
        seq_h1 = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in seq_h]))
        len = seq_h1.shape[1]  # è·å–åºåˆ—é•¿åº¦
        relation_emb_gcn = torch.sum(seq_h1, 1)  # [b,d]
        DA = torch.mm(D, A).float()  # [b,b]
        relation_emb_gcn = torch.mm(DA, relation_emb_gcn)  # [b,d]
        relation_emb_gcn = relation_emb_gcn.unsqueeze(1).expand(relation_emb_gcn.shape[0], len,
                                                                relation_emb_gcn.shape[1])  # [b,s,d]

        target_emb = self.w_f(target_embedding)

        alpha_line = self.get_alpha(x=target_emb)
        q = target_emb  # [b,1,d]
        k = relation_emb_gcn  # [b,1,d]
        v = relation_emb_gcn  # [b,1,d]

        # è°ƒç”¨tglobal_attentionæ–¹æ³•è®¡ç®—æ³¨æ„åŠ›åŠ æƒ:SparseTargetAttentionMechanism
        line_c = self.tglobal_attention(q, k, v, alpha_ent=alpha_line)  # [b,1,d]
        # ä½¿ç”¨seluæ¿€æ´»å‡½æ•°å¹¶å»é™¤å•ç»´åº¦
        c = torch.selu(line_c).squeeze()
        # æ ‡å‡†åŒ–
        l_c = (c / torch.norm(c, dim=-1).unsqueeze(1))

        return l_c  # [b,d]


class SessionGraph(Module):
    def __init__(self, opt, n_node):
        super(SessionGraph, self).__init__()
        self.dataset = opt.dataset
        self.hidden_size = opt.hiddenSize
        self.nei_n = opt.nei_n
        self.n_node = n_node
        self.batch_size = opt.batchSize
        self.nonhybrid = opt.nonhybrid
        # èŠ‚ç‚¹åˆå§‹åŒ–åµŒå…¥
        self.embedding = nn.Embedding(self.n_node, self.hidden_size, padding_idx=0, max_norm=1.5)
        # ä½ç½®åˆå§‹åŒ–åµŒå…¥: ä¸ºä»€ä¹ˆè®¾ç½®æˆä¸‰ç™¾
        self.pos_embedding = nn.Embedding(300, self.hidden_size, padding_idx=0, max_norm=1.5)
        # å®šä¹‰
        # self.gnn = GNN(self.hidden_size)
        self.gnn = GNN(self.hidden_size, 0.5)  # drop0.5
        # mambaç›¸å…³å®šä¹‰
        self.config = Config(model=Mamba4Rec, config_file_list=['config.yaml'])
        self.mamba = trans_to_cuda(Mamba4Rec(opt, self.config))  # mambaé…ç½®

        # Sparse Graph Attention
        self.is_dropout = True
        self.w = 20
        dim = self.hidden_size * 2
        self.dim = dim
        self.LN = nn.LayerNorm(dim)
        self.LN2 = nn.LayerNorm(self.hidden_size)
        self.dropout = nn.Dropout(0.2)
        self.activate = F.relu
        self.atten_w0 = nn.Parameter(torch.Tensor(1, dim))
        self.atten_w1 = nn.Parameter(torch.Tensor(dim, dim))
        self.atten_w2 = nn.Parameter(torch.Tensor(dim, dim))
        self.atten_bias = nn.Parameter(torch.Tensor(dim))
        self.attention_mlp = nn.Linear(dim, dim)
        self.alpha_w = nn.Linear(dim, 1)
        self.self_atten_w1 = nn.Linear(dim, dim)
        self.self_atten_w2 = nn.Linear(dim, dim)
        self.linear2_1 = nn.Linear(2 * dim, dim, bias=True)

        # å¤šå¤´æ³¨æ„åŠ›å‚æ•°
        self.num_attention_heads = opt.num_attention_heads
        self.attention_head_size = int(dim / self.num_attention_heads)
        self.multi_alpha_w = nn.Linear(self.attention_head_size, 1)

        # é‚»å±…æŸ¥æ‰¾æ¨¡å—
        self.FindNeighbor = FindNeighbors(self.hidden_size,self.nei_n)
        self.w_ne = opt.w_ne
        self.gama = opt.gama

        # å…³ç³»å›¾å·ç§¯
        self.RelationGraph = RelationGAT(self.batch_size, self.hidden_size)
        self.w_f = nn.Linear(4 * self.hidden_size, self.hidden_size)
        self.linear_one = nn.Linear(2 * self.hidden_size, 2 * self.hidden_size, bias=True)
        self.linear_two = nn.Linear(2 * self.hidden_size, 2 * self.hidden_size, bias=True)
        self.linear_three = nn.Linear(2 * self.hidden_size, 1, bias=False)
        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)
        self.LayerNorm = LayerNorm(2 * self.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(0.2)
        self.loss_function = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)
        self.reset_parameters()

    # æ¨¡å‹å‚æ•°åˆå§‹åŒ–
    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)

    # æ·»åŠ ä½ç½®åµŒå…¥
    def add_position_embedding(self, sequence):

        batch_size = sequence.shape[0]  # b
        len = sequence.shape[1]  # s

        position_ids = torch.arange(len, dtype=torch.long, device=sequence.device)  # [s,]
        position_ids = position_ids.unsqueeze(0).expand_as(sequence)  # [b,s]
        position_embeddings = self.pos_embedding(position_ids)  # [b,s,d]
        item_embeddings = self.embedding(sequence)

        sequence_emb = torch.cat((item_embeddings, position_embeddings), -1)
        sequence_emb = self.LayerNorm(sequence_emb)

        return sequence_emb

    # å»å™ªå‡½æ•°
    def denoise(self, alpha):  # [b, s+1, s+1]
        batch_size = alpha.shape[0]
        seq_len = alpha.shape[1]
        alpha_avg = torch.mean(alpha, 2, keepdim=True).expand(batch_size, seq_len,
                                                              seq_len)  # å¹³å‡æ³¨æ„åŠ›æƒé‡ [b,s+1]->[b,s+1,s+1]
        alpha_mask = alpha - 0.1 * alpha_avg

        # ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™å¤§äº0çš„å€¼ï¼Œå…¶ä½™ç½®0
        alpha_out = torch.where(alpha_mask > 0, alpha, trans_to_cuda(torch.tensor([0.])))
        return alpha_out

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºå¢å¼ºç›®æ ‡åµŒå…¥å‘é‡target_embï¼ŒåŸºäºlast_embï¼ˆä¸Šä¸€æ¬¡ç‚¹å‡»çš„åµŒå…¥ï¼‰çš„ç›¸ä¼¼åº¦
    def enhanceTarget(self, last_emb, target_emb):  # [b,d],[b,d]

        # è®¡ç®—åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦
        def compute_sim(last_emb, target_emb):
            fenzi = torch.matmul(last_emb, target_emb.permute(1, 0))  # 512*512
            fenmu_l1 = torch.sum(last_emb * last_emb + 0.000001, 1)
            fenmu_l2 = torch.sum(target_emb * target_emb + 0.000001, 1)
            fenmu_l1 = torch.sqrt(fenmu_l1).unsqueeze(1)
            fenmu_l2 = torch.sqrt(fenmu_l2).unsqueeze(1)
            fenmu = torch.matmul(fenmu_l1, fenmu_l2.permute(1, 0))
            cos_sim = fenzi / fenmu  # 512*512
            cos_sim = nn.Softmax(dim=-1)(cos_sim)
            return cos_sim  # [b,b]

        # æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°è°ƒæ•´target_emb
        def compute_pos(batch_size, cos_sim):
            gama = self.gama
            scores = torch.sum(cos_sim, 1)  # [b,]
            value = torch.mean(scores) * gama  # [1,] ç›¸ä¼¼åº¦å¾—åˆ†çš„å‡å€¼
            for index in range(batch_size):
                target_emb[index] = torch.where(scores[index] - value > 0,
                                                self.linear2_1(torch.cat([target_emb[index], last_emb[index]], 0)),
                                                target_emb[index])

        # å‘é‡å‹ç¼©ï¼šå»é™¤å•ç»´åº¦
        target_emb = target_emb.squeeze()  # [b,2d]
        #  è°ƒç”¨compute_simå‡½æ•°è®¡ç®—last_embå’Œtarget_embä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        cos_sim = compute_sim(last_emb, target_emb)  # [b,b]
        batch_size = last_emb.shape[0]  # b
        mask = trans_to_cuda(torch.Tensor(np.diag([1] * batch_size)))  # [b,b] æ„é€ å¯¹è§’çŸ©é˜µ
        scores = cos_sim * mask  # åªæœ‰å¯¹è§’çº¿ä¸Šæœ‰å€¼ [b,b] [0.4,0,0,0,0][0,0.5,0,0,0]
        # è¿™ä¸ªå‡½æ•°ä¼šæ ¹æ®scoresæ›´æ–°target_emb
        compute_pos(batch_size, scores)
        up_target = target_emb.unsqueeze(1)  # [b,1,d]
        # è¿”å›ç»è¿‡ç›¸ä¼¼åº¦åŠ æƒå¢å¼ºåçš„target_emb
        return up_target

    # è®¡ç®—æ³¨æ„åŠ›æƒé‡
    def get_alpha(self, x=None, seq_len=70, number=None):  # x[b,1,d], seq = lenä¸ºæ¯ä¸ªä¼šè¯åºåˆ—ä¸­æœ€åä¸€ä¸ªå…ƒç´ 
        if number == 0:
            alpha_ent = torch.sigmoid(self.alpha_w(x)) + 1  # [b,1,1]
            alpha_ent = self.add_value(alpha_ent).unsqueeze(1)  # [b,1,1]
            alpha_ent = alpha_ent.expand(-1, seq_len, -1)  # [b,s+1,1]
            return alpha_ent
        if number == 1:  # x[b,1,d]
            alpha_global = torch.sigmoid(self.alpha_w(x)) + 1  # [b,1,1]
            alpha_global = self.add_value(alpha_global)
            return alpha_global

    # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œé€‚ç”¨äºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    def get_alpha2(self, x=None, seq_len=70):  # x [b,n,d/n]
        alpha_ent = torch.sigmoid(self.multi_alpha_w(x)) + 1  # [b,n,1]
        alpha_ent = self.add_value(alpha_ent).unsqueeze(2)  # [b,n,1,1]
        alpha_ent = alpha_ent.expand(-1, -1, seq_len, -1)  # [b,n,s,1]
        return alpha_ent

    def add_value(self, value):

        mask_value = (value == 1).float()
        value = value.masked_fill(mask_value == 1, 1.00001)
        return value

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    # ä¿®æ”¹
    def Multi_Self_attention(self, q, k, v, sess_len):
        # q,k,v([512, 40, 200])
        is_dropout = True
        if is_dropout:
            q_ = self.dropout(self.activate(self.attention_mlp(q)))  # [b,s+1,d]
        else:
            q_ = self.activate(self.attention_mlp(q))

        query_layer = self.transpose_for_scores(q_)
        key_layer = self.transpose_for_scores(k)
        value_layer = self.transpose_for_scores(v)
        # print(query_layer.size())   # ([512, 5, 40, 40])

        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        alpha_ent = self.get_alpha2(query_layer[:, :, -1, :], seq_len=sess_len)

        attention_probs = entmax_bisect(attention_scores, alpha_ent, dim=-1)

        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.dim,)
        att_v = context_layer.view(*new_context_layer_shape)

        if is_dropout:
            att_v = self.dropout(self.self_atten_w2(self.activate(self.self_atten_w1(att_v)))) + att_v
        else:
            att_v = self.self_atten_w2(self.activate(self.self_atten_w1(att_v))) + att_v

        att_v = self.LN(att_v)
        c = att_v[:, -1, :].unsqueeze(1)  # [b,d]->[b,1,d]
        x_n = att_v[:, :-1, :]  # [b,s,d]
        # print(c.size())    #ï¼ˆ512,1,200ï¼‰
        # print(x_n.size())    #ï¼ˆ512,39,200ï¼‰
        return c, x_n

    # def self_attention(self, q, k, v, mask=None, alpha_ent=1):
    #     is_dropout = True
    #     if is_dropout:
    #         q_ = self.dropout(self.activate(self.attention_mlp(q)))  # [b,s+1,d]
    #     else:
    #         q_ = self.activate(self.attention_mlp(q))
    #     scores = torch.matmul(q_, k.transpose(1, 2)) / math.sqrt(self.dim)  # [b,s+1,d]x[b,d,s+1] = [b,s+1,s+1]
    #     if mask is not None:
    #         mask = mask.unsqueeze(1).expand(-1, q.size(1), -1)
    #         scores = scores.masked_fill(mask == 0, -np.inf)
    #     alpha = entmax_bisect(scores, alpha_ent, dim=-1)  # [b,s+1,s+1] æ³¨æ„å‘é‡
    #     # alpha2 = F.softmax(scores, dim=-1)
    #
    #     att_v = torch.matmul(alpha, v)  #[b,s+1,d]
    #
    #
    #     if is_dropout:
    #         att_v = self.dropout(self.self_atten_w2(self.activate(self.self_atten_w1(att_v)))) + att_v
    #     else:
    #         att_v = self.self_atten_w2(self.activate(self.self_atten_w1(att_v))) + att_v
    #     att_v = self.LN(att_v)
    #     c = att_v[:, -1, :].unsqueeze(1)
    #     x_n = att_v[:, :-1, :]
    #     print(666)
    #     print(c.size())
    #     print(x_n.size())
    #     return c, x_n

    def global_attention(self, target, k, v, mask=None, alpha_ent=1):
        alpha = torch.matmul(torch.relu(k.matmul(self.atten_w1) + target.matmul(self.atten_w2) + self.atten_bias),
                             self.atten_w0.t())
        if mask is not None:  # [b,s]
            mask = mask.unsqueeze(-1)
            alpha = alpha.masked_fill(mask == 0, -np.inf)
        alpha = entmax_bisect(alpha, alpha_ent, dim=1)
        c = torch.matmul(alpha.transpose(1, 2), v)
        return c

    # [b,d], [b,d]
    def decoder(self, global_s, target_s):
        if self.is_dropout:
            c = self.dropout(torch.selu(self.w_f(torch.cat((global_s, target_s), 2))))
        else:
            c = torch.selu(self.w_f(torch.cat((global_s, target_s), 2)))  # [b,1,4d]

        c = c.squeeze()  # [b,d]
        l_c = (c / torch.norm(c, dim=-1).unsqueeze(1))
        return l_c

    def compute_scores(self, hidden, mask, target_emb, att_hidden, relation_emb):  # Dual_att[b,d], Dual_g[b,d]
        # htä¸ºlocal_embedding å–hiddenæœ€åä¸€ä¸ªå…ƒç´ 
        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size
        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size
        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size
        # ä½¿ç”¨sigmoidè®¡ç®—åŠ æƒå’Œ
        sess_global = torch.sigmoid(q1 + q2)  # [b,s,d]

        # Sparse Global Attentionï¼Œè®¡ç®—å…¨å±€æ³¨æ„åŠ›æƒé‡
        alpha_global = self.get_alpha(x=target_emb, number=1)  # [b,1,2d]
        # å®šä¹‰q,k,vè®¡ç®—å…¨å±€æ³¨æ„åŠ›
        q = target_emb
        k = att_hidden  # [b,s,2d]
        v = sess_global  # [b,s,2d]
        # ä½¿ç”¨global_attentionè®¡ç®—å…¨å±€æ³¨æ„åŠ›çš„è¾“å‡º
        global_c = self.global_attention(q, k, v, mask=mask, alpha_ent=alpha_global)
        # ä½¿ç”¨decoderå°†å…¨å±€æ³¨æ„åŠ›çš„è¾“å‡ºå’Œtarget_embè½¬æ¢ä¸ºsess_final
        sess_final = self.decoder(global_c, target_emb)
        # SICï¼šé€šè¿‡findneighboræ–¹æ³•æ‰¾åˆ°é‚»å±…å¹¶æ›´æ–°sess_final
        neighbor_sess = self.FindNeighbor(sess_final + relation_emb)
        sess_final = sess_final + neighbor_sess

        b = self.embedding.weight[1:] / torch.norm(self.embedding.weight[1:], dim=-1).unsqueeze(1)
        # è®¡ç®—å¾—åˆ†
        scores = self.w * torch.matmul(sess_final, b.transpose(1, 0))  # [b,d]x[d,n] = [b,n]
        return scores

    def forward(self, inputs, A, alias_inputs, A_hat, D_hat):  # inputs[b,s], A[b,s,s]
        # å¼•å…¥å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ğ‘‹ğ‘¡ = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (ğ‘‰ğ‘¡ , ğ‘ƒğ‘¡ )
        seq_emb = self.add_position_embedding(inputs)  # [b,s,2d]
        # gnnå±‚ï¼Œä½¿ç”¨GGNNä½œä¸ºåˆå§‹ç¼–ç å™¨
        # hidden = self.gnn(A, seq_emb)  # (b,s,2d)
        hidden = self.gnn(seq_emb, A)  # (b,s,2d)
        # print(hidden.size())
        get = lambda i: hidden[i][alias_inputs[i]]
        seq_hidden_gnn = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])  # [b,s,2d]
        # åˆ›å»ºé¢å¤–èŠ‚ç‚¹ğ»ğ‘¡ = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (ğ»ğ‘¡ , ğ‘¡ğ‘› ),
        zeros = torch.cuda.FloatTensor(seq_hidden_gnn.shape[0], 1, self.dim).fill_(0)  # [b,1,d]
        # å°†èŠ‚ç‚¹åµŒå…¥å’Œç©ºç™½èŠ‚ç‚¹æ²¿ç€åºåˆ—é•¿åº¦ç»´åº¦æ‹¼æ¥ï¼ˆå°†ç©ºç™½èŠ‚ç‚¹æ·»åŠ åˆ°åºåˆ—æœ«å°¾ï¼‰
        session_target = torch.cat([seq_hidden_gnn, zeros], 1)  # [b,s+1,d]
        # è·å¾—ç¬¬ä¸€ç»´çš„å¤§å°ï¼ˆåºåˆ—é•¿åº¦ï¼‰
        sess_len = session_target.shape[1]
        # target_emb = session_target[:, -1, :].unsqueeze(1)  # [b,d]->[b,1,d]
        # x_n = session_target[:, :-1, :]  # [b,s,d]
        target_emb, x_n = self.mamba(session_target, sess_len)
        # # ä¼šè¯å›¾ç¨€ç–è‡ªæ³¨æ„åŠ›æœºåˆ¶
        # target_emb, x_n = self.Multi_Self_attention(session_target, session_target, session_target, sess_len)
        # å…³ç³»å›¾ç¨€ç–è‡ªæ³¨æ„åŠ›æœºåˆ¶
        # self.RelationGraph = RelationGAT(self.batch_size, self.hidden_size)
        relation_emb = self.RelationGraph(self.embedding.weight, inputs, A_hat, D_hat, target_emb)

        return seq_hidden_gnn, target_emb, x_n, relation_emb


def trans_to_cuda(variable):
    if torch.cuda.is_available():
        return variable.cuda()
    else:
        return variable


def trans_to_cpu(variable):
    if torch.cuda.is_available():
        return variable.cpu()
    else:
        return variable


def forward(model, i, data):
    # è·å¾—ç¬¬iä¸ªæ‰¹æ¬¡çš„æ•°æ®åˆ‡ç‰‡
    alias_inputs, A, items, mask, targets = data.get_slice(i)  # å¾—åˆ°ç¢ç‰‡æ•°æ®ï¼šbatchä¸­çš„å€¼
    # è®¡ç®—é¡¹ç›®é‡å éƒ¨åˆ†
    A_hat, D_hat = data.get_overlap(items)

    A_hat = trans_to_cuda(torch.Tensor(A_hat))
    D_hat = trans_to_cuda(torch.Tensor(D_hat))
    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())
    items = trans_to_cuda(torch.Tensor(items).long())
    A = trans_to_cuda(torch.Tensor(A).float())
    mask = trans_to_cuda(torch.Tensor(mask).long())

    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­
    hidden, target_emb, att_hidden, relation_emb = model(items, A, alias_inputs, A_hat, D_hat)
    # è°ƒç”¨æ¨¡å‹çš„æ–¹æ³•è®¡ç®—å¾—åˆ†
    scores = model.compute_scores(hidden, mask, target_emb, att_hidden, relation_emb)

    return targets, scores


def train_test(model, train_data, test_data):
    model.scheduler.step()
    print('start training: ', datetime.datetime.now())
    model.train()
    total_loss = 0.0
    # ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
    slices = train_data.generate_batch(model.batch_size)
    # éå†æ¯ä¸ªæ‰¹æ¬¡
    for i, j in zip(slices, np.arange(len(slices))):
        model.optimizer.zero_grad()
        # å‰å‘ä¼ æ’­
        targets, scores = forward(model, i, train_data)
        targets = trans_to_cuda(torch.Tensor(targets).long())
        # è®¡ç®—æŸå¤±
        loss = model.loss_function(scores, targets - 1)
        # åå‘ä¼ æ’­
        loss.backward()
        model.optimizer.step()
        # ç´¯è®¡æ€»æŸå¤±
        total_loss = total_loss + loss
        if j % int(len(slices) / 5 + 1) == 0:
            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()), flush=True)
    print('\tLoss:\t%.3f' % total_loss)

    print('start predicting: ', datetime.datetime.now())
    model.eval()
    hit, mrr = [], []
    # ç”Ÿæˆæµ‹è¯•æ‰¹æ¬¡
    slices = test_data.generate_batch(model.batch_size)
    # éå†æ¯ä¸ªæµ‹è¯•æ‰¹æ¬¡
    for i in slices:
        # å‰å‘ä¼ æ’­
        targets, scores = forward(model, i, test_data)
        sub_scores = scores.topk(20)[1]
        sub_scores = trans_to_cpu(sub_scores).detach().numpy()
        for score, target, mask in zip(sub_scores, targets, test_data.mask):
            hit.append(np.isin(target - 1, score))
            if len(np.where(score == target - 1)[0]) == 0:
                mrr.append(0)
            else:
                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))
    hit = np.mean(hit) * 100
    mrr = np.mean(mrr) * 100
    return hit, mrr



